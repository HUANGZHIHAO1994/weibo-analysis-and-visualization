# -*- coding: utf-8 -*
'''
åŒ¹é…contentã€commentå¹¶åˆæ­¥æ¸…ç†æ•°æ®
ç”¨äºä½œè¯äº‘wc.pyã€åœ°å›¾graph.pyã€Tf-idfæ–‡æœ¬èšç±»cluster_tfidfã€Word2Vecæ–‡æœ¬èšç±»cluster_w2v
å°†æ¯ä¸ªåˆ†ç±»å…³é”®è¯å½¢å¼å˜ä¸ºï¼š
[
  [url1, content1åŸæ–‡, [content1åˆ†è¯],[comment1],...,[comment_n]],
  [url2, content2åŸæ–‡, [content2åˆ†è¯],[comment1],...,[comment_m]],
  ...
]
'''

import codecs
import json
import pandas as pd
import jieba
import pickle
import re
from langconv import *

def Traditional2Simplified(sentence):
    '''
    å°†sentenceä¸­çš„ç¹ä½“å­—è½¬ä¸ºç®€ä½“å­—
    :param sentence: å¾…è½¬æ¢çš„å¥å­
    :return: å°†å¥å­ä¸­ç¹ä½“å­—è½¬æ¢ä¸ºç®€ä½“å­—ä¹‹åçš„å¥å­
    '''
    sentence = Converter('zh-hans').convert(sentence)
    return sentence

def Sent2Word(sentence):
    """Turn a sentence into tokenized word list and remove stop-word

    Using jieba to tokenize Chinese.

    Args:
        sentence: A string.

    Returns:
        words: A tokenized word list.
    """
    global stop_words

    words = jieba.cut(sentence)
    words = [w for w in words if w not in stop_words]

    return words


def Match(comment, content):
    """åŒ¹é…å¾®åšå†…å®¹å’Œå¾®åšè¯„è®ºæ•°æ®ï¼Œå¹¶å°†æ˜æ˜¾çš„å¹¿å‘Šå¾®åšå‰”é™¤

    Args:
        comment_example:
      [
      {'_id': 'C_4322161898716112', 'crawl_time': '2019-06-01 20:35:36', 'weibo_url': 'https://weibo.com/1896820725/H9inNf22b', 'comment_user_id': '6044625121', 'content': 'æ²¡é—®é¢˜ï¼Œ', 'like_num': {'$numberInt': '0'}, 'created_at': '2018-12-28 11:19:21'},...
      ]

        content_example:
      [
      {'_id': '1177737142_H4PSVeZWD', 'keyword': 'Aè‚¡', 'crawl_time': '2019-06-01 20:31:13', 'weibo_url': 'https://weibo.com/1177737142/H4PSVeZWD', 'user_id': '1177737142', 'created_at': '2018-11-29 03:02:30', 'tool': 'Android', 'like_num': {'$numberInt': '0'}, 'repost_num': {'$numberInt': '0'}, 'comment_num': {'$numberInt': '0'}, 'image_url': 'http://wx4.sinaimg.cn/wap180/4632d7b6ly1fxod61wktyj20u00m8ahf.jpg', 'content': '#aè‚¡è§‚ç‚¹# é²å¨å°”ä¸»å¸­æˆ–æ˜¯å› ä¸ºè¢«ç‰¹æœ—æ™®æ€»ç»Ÿç‚¹åæ‰¹è¯„åèŒç”Ÿæ‚”æ”¹ä¹‹æ„ï¼Œä»Šæ™šä¸€ç•ªè®²è¯è¢«å¸‚åœºè§£è¯»ä¸ºç¾è”å‚¨æˆ–æš‚åœåŠ æ¯æ­¥ä¼ã€‚ç¾å…ƒæŒ‡æ•°åº”å£°ä¸‹æŒ«ï¼Œç¾è‚¡åŠé‡‘å±è´µé‡‘å±ä»·æ ¼å¤§å¹…ä¸Šæ‰¬ï¼ŒA50è¡¨ç°ä¹Ÿå¹¶ä¸é€Šè‰²å¤ªå¤šã€‚å¯¹æ˜å¤©Aè‚¡æˆ–æœ‰ç§¯æå½±å“ï¼Œåå¼¹æˆ–èƒ½å¾—ä»¥å»¶ç»­ã€‚ [ç»„å›¾å…±2å¼ ]'},...
      ]

    Returns:
        å…¶å®æ²¡æœ‰returnï¼Œå½¢æˆå¦‚ä¸‹æ ¼å¼çš„pklæ–‡ä»¶ï¼š
        [
        [url1, content1åŸæ–‡, [content1åˆ†è¯],[comment1],...,[comment_n]],
        [url2, content2åŸæ–‡, [content2åˆ†è¯],[comment1],...,[comment_m]],
        ...
        ]
    """
    content_comment = []
    advertisement = ["ç‹è€…è£è€€", "åˆ¸å", "å”®ä»·", 'Â¥', "ï¿¥", 'ä¸‹å•']

    for k in range(0, len(content)):
        judge = []
        print('Processing train ', k)
        content[k]['content'] = Traditional2Simplified(content[k]['content'])
        for adv in advertisement:
            if adv in content[k]['content']:
                judge.append("True")
                break
        if re.search(r"ä¹°.*èµ .*", content[k]['content']):
            judge.append("True")
            continue
        # é€šè¿‡ä¸Šé¢çš„ä¸¤ç§æ¨¡å¼åˆ¤æ–­æ˜¯ä¸æ˜¯å¹¿å‘Š
        if "True" not in judge:
            comment_list = []
            url = content[k]['weibo_url']
            comment_list.append(url)
            comment_list.append(content[k]['content'])
            # æ•°æ®æ¸…æ´—
            a2 = re.compile(r'#.*?#')
            content[k]['content'] = a2.sub('', content[k]['content'])
            a3 = re.compile(r'\[ç»„å›¾å…±.*å¼ \]')
            content[k]['content'] = a3.sub('', content[k]['content'])
            a4 = re.compile(r'http:.*')
            content[k]['content'] = a4.sub('', content[k]['content'])
            a5 = re.compile(r'@.*? ')
            content[k]['content'] = a5.sub('', content[k]['content'])
            a6 = re.compile(r'\[.*?\]')
            content[k]['content'] = a6.sub('', content[k]['content'])
            comment_list.append(Sent2Word(content[k]['content']))
            for i in comment:
                if i['weibo_url'] == url:    #  é€šè¿‡URLåŒ¹é…contentå’Œcomment
                    a1 = re.compile(r'å›å¤@.*:')
                    i['content'] = a1.sub('', i['content'])
                    i['content'] = Traditional2Simplified(i['content'])
                    i['content'] = a2.sub('', i['content'])
                    i['content'] = a4.sub('', i['content'])
                    i['content'] = a5.sub('', i['content'])
                    i['content'] = a6.sub('', i['content'])
                    comment_list.append(Sent2Word(i['content']))
            content_comment.append(comment_list)

    pickle.dump(content_comment, open('./Agu.pkl', 'wb'))


if __name__ == '__main__':

    print("åœç”¨è¯è¯»å–")
    stop_words = [w.strip() for w in open('./dict/å“ˆå·¥å¤§åœç”¨è¯è¡¨.txt', 'r', encoding='UTF-8').readlines()]
    stop_words.extend(['\n', '\t', ' ', 'å›å¤', 'è½¬å‘å¾®åš', 'è½¬å‘', 'å¾®åš', 'ç§’æ‹', 'ç§’æ‹è§†é¢‘', 'è§†é¢‘', "ç‹è€…è£è€€", "ç‹è€…", "è£è€€"])
    for i in range(128000, 128722 + 1):
        stop_words.extend(chr(i))
    stop_words.extend(['Aè‚¡'])

    # ç‰¹æ®Šå­—ç¬¦

    # print(ord("ğŸ"))
    # print(chr(77823))
    # print(hex(128300))
    # print(0x1E000)
    # # E000-F8FF 122880-129279  # è‡ªè¡Œä½¿ç”¨å€åŸŸ (Private Use Zone)
    # # F900-FAFF 129280-129791
    # count = 0
    # for i in range(128000, 129279+1):
    #     count += 1
    #     if count <100:
    #         print(chr(i))
    #     else:
    #         break

    #  jsonæ–‡æ¡£è¯»æ³•
    print("commentè¯»å–")
    f = codecs.open('./Agu_comment.json', 'r', 'UTF-8-sig')
    comment = []
    for i in f.readlines():
        try:
            comment.append(eval(i))
        except:
            continue
    # comment = [json.loads(i) for i in f.readlines()]  # json.loadsä¹Ÿè¡Œ
    f.close()
    # print(comment)

    '''
      comment_example:
      [
      {'_id': 'C_4322161898716112', 'crawl_time': '2019-06-01 20:35:36', 'weibo_url': 'https://weibo.com/1896820725/H9inNf22b', 'comment_user_id': '6044625121', 'content': 'æ²¡é—®é¢˜ï¼Œ', 'like_num': {'$numberInt': '0'}, 'created_at': '2018-12-28 11:19:21'},...
      ]
    '''

    # pandas csvæ–‡æ¡£è¯»æ³•
    # df_weibo = pd.read_csv('comment.csv', sep=',', quotechar='"', error_bad_lines=False)
    # print(df_weibo.head())

    print("contentè¯»å–")
    f = codecs.open('./Agu_content.json', 'r', 'UTF-8-sig')
    content = []
    for i in f.readlines():
        try:
            content.append(eval(i))
        except:
            continue
    # jinkou = [json.loads(i) for i in f.readlines()]  # json.loadsä¹Ÿè¡Œ
    f.close()
    # print(content)

    '''
      content_example:
      [
      {'_id': '1177737142_H4PSVeZWD', 'keyword': 'Aè‚¡', 'crawl_time': '2019-06-01 20:31:13', 'weibo_url': 'https://weibo.com/1177737142/H4PSVeZWD', 'user_id': '1177737142', 'created_at': '2018-11-29 03:02:30', 'tool': 'Android', 'like_num': {'$numberInt': '0'}, 'repost_num': {'$numberInt': '0'}, 'comment_num': {'$numberInt': '0'}, 'image_url': 'http://wx4.sinaimg.cn/wap180/4632d7b6ly1fxod61wktyj20u00m8ahf.jpg', 'content': '#aè‚¡è§‚ç‚¹# é²å¨å°”ä¸»å¸­æˆ–æ˜¯å› ä¸ºè¢«ç‰¹æœ—æ™®æ€»ç»Ÿç‚¹åæ‰¹è¯„åèŒç”Ÿæ‚”æ”¹ä¹‹æ„ï¼Œä»Šæ™šä¸€ç•ªè®²è¯è¢«å¸‚åœºè§£è¯»ä¸ºç¾è”å‚¨æˆ–æš‚åœåŠ æ¯æ­¥ä¼ã€‚ç¾å…ƒæŒ‡æ•°åº”å£°ä¸‹æŒ«ï¼Œç¾è‚¡åŠé‡‘å±è´µé‡‘å±ä»·æ ¼å¤§å¹…ä¸Šæ‰¬ï¼ŒA50è¡¨ç°ä¹Ÿå¹¶ä¸é€Šè‰²å¤ªå¤šã€‚å¯¹æ˜å¤©Aè‚¡æˆ–æœ‰ç§¯æå½±å“ï¼Œåå¼¹æˆ–èƒ½å¾—ä»¥å»¶ç»­ã€‚ [ç»„å›¾å…±2å¼ ]'},...
      ]
    '''

    Match(comment, content)


